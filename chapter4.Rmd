---
output:
  html_document:
    theme: cosmo
    toc: true
    toc_depth: 2
    fig_caption: true
    fig_width: 13
    fig_height: 9
---

# Chapter 4 - Clustering and classification

***

## Overview of the data

<br /> In this analysis we will be using data set called _"Boston"_, which can be found from library called _"MASS"_ ([source](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html)). Let us look closer how does the data look like.
<br /><br /> 

```{r}
date()

#Accessing the MASS package
library(MASS)

#Loading the data
data("Boston")

#Exploring the dataset
str(Boston)

summary(Boston)


```


<br /> The Boston data frame has 506 rows and 14 columns. The 14 columns are:

- crim > per capita crime rate by town.
- zn > proportion of residential land zoned for lots over 25,000 sq.ft.
- indus > proportion of non-retail business acres per town.
- chas > Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
- nox > nitrogen oxides concentration (parts per 10 million).
- rm > average number of rooms per dwelling.
- age > proportion of owner-occupied units built prior to 1940.
- dis > weighted mean of distances to five Boston employment centres.
- rad > index of accessibility to radial highways.
- tax > full-value property-tax rate per \$10,000.
- ptratio > pupil-teacher ratio by town.
- black > 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
- lstat > lower status of the population (percent).
- medv > median value of owner-occupied homes in \$1000s

Let us next see the **plot matrix** of all the variables.

<br /><br /> 


```{r}

#Plotting matrix of the variables
pairs(Boston)

```


<br /> And then the **correlation matrix** The matrix is easy way to examine the correlations between all variables.
<br /><br /> 


```{r}

library(tidyr)
library(corrplot)

#Calculating the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)

#Priunting the correlation matrix
cor_matrix

#Visualizing the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)


```


<br /> The correlations exceeding 0.6 are between: _crim - rad_ (0.63), _zn - dis_ (0.66), _indus - nox_ (0.76), _indus - age_ (0.64), _indus - dis_ (0.6), _indus - tax_ (0.72), _nox - age_ (0.73), _nox - rad_ (0.61), _nox - tax_ (0.67), _rad - tax_ (0.91).

- The **highest correlation** is between _rad - tax_ **(0.91)**, ergo between the index of accessibility to radial highways and a full-value property-tax rate per \$10,000.

Correlations lower than -0.6 are between: _indus - dis_ (-0.71), _nox - dis_ (-0.77), _age - dis_ (-0.75).

- The **lowest correlation** is between _nox - dis_ **(-0.77)**, ergo between the nitrogen oxides concentration (parts per 10 million) and weighted mean of distances to five Boston employment centres.
<br /><br /> 


## Scaling and standardizing

<br />
For later use, we need to scale our data. In scaling, the column means will be subtracted from the corresponding columns and then the difference will be divided with standard deviation. 

As the _"Boston"_ data set contains only numerical values, the whole data needs to be standardized first using function scale().
<br /><br /> 


```{r}
#Let us center and standardize variables
boston_scaled <- scale(Boston)

#Summaries of the scaled variables
summary(boston_scaled)

#The class of the boston_scaled object
class(boston_scaled)

```

<br /> Standardizing the data leads to setting the **mean** of all the variables to **0**. In order to maximize compatibility of the variables. 

The class of the scaled _Boston_ data set is an array matrix.

Later on, we will want the data to be a _data frame_. Thereby, we will next convert the scaled _Boston_-data set to a data frame format.
<br /><br /> 


```{r}

#Changing the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

```


<br /> We will be focusing on the **crime rate** in the Boston (data set). To use and analyse the variable crim(e) we need to create a categorical variable from a continuous one. Thus, let us change crim to a crime (rate per capita by town). We will cut the variable by quantiles (used as break points) to get the high, low and middle rates of crime into their own categories (> categorical variable).
<br /><br /> 


```{r}

#Creating a quantile vector of 'crim' and printing it
bins <- quantile(boston_scaled$crim)
bins

#Creating a categorical variable 'crime' according to the quantiles set in bins
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low","med_low","med_high","high"))

#Let us look at the table of the new factor crime
table(crime)

#Dropping original variable 'crim' from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

#Replacing removed 'crim' with the new categorical value 'crime' to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

<br /> The categorized crime data consists of as even categories as possible: 

- 0%-25%  _low_  > (127)
- 25%-50%  _med_low_  > (126)
- 50%-75%  _med_high_  > (126)
- 75%-100%  _high_  > (127)

<br /> Next we will divide our data set to 'train' and 'test' sets. 80% of the data will go to the 'train' set.
<br />

```{r}

#The number of rows in the Boston data set 
n <- nrow(boston_scaled)

#Choosing randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

#Creating 'train' set (80% of the rows)
train <- boston_scaled[ind,]

#Creating 'test' set (20% of the rows)
test <- boston_scaled[-ind,]

#Let us save the correct classes from 'test' data
correct_classes <- test$crime

#And remove the crime variable from 'test' data
test <- dplyr::select(test, -crime)


```

<br />

## Linear discriminant analysis

<br /> Now we will apply the **linear discriminant analysis** on the train set. [LDA](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) is a classification (and dimension reduction) method used to find a linear combination of features characterizing 2 or more classes of objects. The resulting combinations may be used as a linear classifier or for dimensionality reduction before later classification. 

In the LDA below the categorical crime rate is used as the target variable and all the other variables in the data set act as predictor variables. Then there is a LDA biplot.
<br /><br /> 

```{r}

#A linear discriminant analysis (LDA)
#Crime rate as target variable, all the others (.) as predictor variables
lda.fit <- lda(crime ~ ., data = train)

#Printing the lda.fit object
lda.fit

#The function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

#Target classes as numeric
classes <- as.numeric(train$crime)

#Plotting the results of lda
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)

```

<br /> In the graph above we can see, that only LD1 separates the classes nicely. LD2 does not give any valuable information related to the separation. The arrow called **'rad'** is the longest and directed towards the **'high'** classes. Also the arrows of variables 'zn' and 'nox' are differing from the others. 'zn' is directed upward and 'nox' downward.
<br /><br /> 


### Predicting classes with LDA

<br /> Let us try to predict the values based on our model. The data was split earlier on in order to have a test set and the correct class labels. Now we will test how the LDA model performs when predicting the classes on test data.

Below, there is a cross tabulation of the results with the crime categories from the test set.
<br /><br /> 


```{r}

#Prediction of the classes with test data
lda.pred <- predict(lda.fit, newdata = test)

#Cross tabulation of the results
table(correct = correct_classes, predicted = lda.pred$class)

```

<br /> The cross tabulation results in table, where correct answers are on the left hand side as rows and predictions as columns. It is good to see that most of the observations have been predicted correctly (numbers in diagonal). However, there are several spots with wrong predictions. The highest number of mistakes in prediction are under 'med_low'. The model has predicted several low and med_high cases as med_low. The exact number is changing after each run due to the randomnes in the script.
<br /><br /> 


## Clustering with K-means

<br />

### Distances between observations

<br />
Let us start by calculating the _(euclidean and manhattan)_ distances between observations. 
<br /><br />

```{r}

#Reloading MASS and Boston data sets
library(MASS)
data('Boston')

#Scaling and standardizing the data set 'Boston'
boston_scaled1 <- scale(Boston)

#Changing the object to data frame
boston_scaled2 <- as.data.frame(boston_scaled1)


#Euclidean distance matrix
dist_eu <- dist(boston_scaled2)

#Let us see the summary of the distances
summary(dist_eu)

#Manhattan distance matrix
dist_man <- dist(boston_scaled2, method = "manhattan")

#Let us now see the summary of the distances again
summary(dist_man)

```

<br /> Now we continue to the **K-means algorithm**. K-means is one of the most used clustering method. The method assigns observations into groups (clusters) based on similarity of the objects. The K-means (function kmeans()) counts the distance matrix (done above) automatically. 
As there are quite a lot of variables, 5 columns (columns 6 to 10) have been paired up to make examination more clear. The first 5 columns are less informative, and thereby the last 5 columns have been chosen.
<br /><br /> 

```{r}

#K-means clustering
km <-kmeans(boston_scaled2, centers = 3)

#Plotting the Boston dataset with clusters
#5 columns (columns 6 to 10) have been paired up to make examination more clear
pairs(boston_scaled2[6:10], col = km$cluster)

```

<br />
As K-means takes the amount of clusters in as an argument, we need to look for the optimal amount of clusters. One way of estimating the optimal number of clusters has been tested below. There we look how the total of within cluster sum of squares (WCSS) acts when the number of clusters change.

K-means may produce different results every time, as it randomly assigns the initial cluster centers. The function set.seed() is used to deal with it.
<br /><br /> 

```{r}
library(ggplot2)

#To ensure that the result does not vary every time
set.seed(123)

#Determining the number of clusters
k_max <- 10

#Calculating the total within sum of squares (twcss)
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled2, k)$tot.withinss})

#Visualizing the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```

<br /> The optimal number of clusters is where the total WCSS drops radically. In the figure above, the most radical drop is just in the beginning and the drop ends at value 2. This indicates, that the **optimal number of clusters would be 2**. Let us run the K-means algorithm again, using the optimal number of clusters (2).
<br /><br /> 


```{r}

#K-means clustering
km <-kmeans(boston_scaled2, centers = 2)

#Plotting the Boston dataset with clusters
pairs(boston_scaled2[6:10], col = km$cluster)

```

<br /> The different variables cause a different shaped clustering. Variables 'rad' and 'tax' cause two clearly separated groups (one in the area of -1 - 0 and the other in the range of 1.5).

'age' and 'dis' cause a more connected clusters, where the first cluster gradually changes to the next one. In 'dis' the first cluster starts from -1 and as the value increases the second cluster begins. in 'age' the first cluster starts from 1 and as the value decreases the second cluster begins.
<br /><br /> 

***

<br /> 

> _Summary of my learning_

<br /> This week I felt slightly lost. I managed very well to do everything that I was asked to do but the deeper understanding is not fully there. I can make graphs and tables and I can tell what I see... But, for example, what do the arrows (of variables) really tell me in the LDA graph? Can the distances on their own be interpret/useful somehow? How do we utilize the two resulting groups from the clustering? Maybe I'll learn the answer's later on. For now, I think I just need to settle on executing the tasks.
<br /><br />

***


Regards,
_Janina_

