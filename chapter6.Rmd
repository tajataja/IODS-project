---
output:
  html_document:
    theme: cosmo
    toc: true
    toc_depth: 2
    fig_caption: true
    fig_width: 9
    fig_height: 5
---

# Chapter 6 - Analysis of longitudinal data

***

## Working with 2 sets of data

<br /> This week we have two datasets to investigate (BPRS and RATS). These datasets are _longitudinal_ and thereby the response variable (and maybe even some or
all of the explanatory variables) is observed several times > _multivariate_. The analysis of repeated measures and longitudinal data is in need of certain models that are able to both **assess the effects** of explanatory variables on the multiple measures of the response variable and account for the **likely correlations** between these measures. 

First part of this week's data analysis will focus on the RATS dataset. We will be looking at some graphical displays of the data and a possible approach to its' analysis. Let us look closer how does the data look like.
<br /><br /> 

## RATS - dataset

<br /> 
The data is from a nutrition study that was conducted in three groups of rats in 1990 by Crowder and Hand. The three groups of rats had different types of diets. For 9 weeks each animal's body weight (in grams) was recorded repeatedly approximately once a week > _repeated measured data_. The interest was to see whether there were any differences in the growth profiles of the three groups. 
<br /><br /> 

```{r}
date()

RATSL = read.table(file="RATSL.txt", sep=",", header=TRUE)

#Factor ID & Group of RATS
RATSL$ID <- factor(RATSL$ID)
RATSL$Group <- factor(RATSL$Group)

#Let us see the dimensions and structure of the dataset
dim(RATSL)
str(RATSL)
```

<br /> There are 176 observations of 5 variables. First there is the ID-number of the rat and the group the animal belongs into. The variable 'Times' has been constructed out of 'times' and it tells how many times the weight has been measured. The variable 'Weight' is the weight of the rat in grams.
<br /><br /> 

### Graphical display

### Individual response profiles

<br /> We shall begin by plotting the weight of the rats over time in the three groups _(1, 2_ and _3_). 
<br /><br />

```{r}
library(GGally)
library(dplyr)
library(corrplot)
library(ggplot2)
library(reshape2)

#Drawing a plot using ggplot
ggplot(RATSL, aes(x = Time, y = Weight, col=Group)) +
  geom_point() +
  labs(x="Time (d)", y="Weight (g)", size="horsepower",
       col="Group")

```

<br /> Above, all the weight profiles of the three groups are presented in one graph. The colouring shows which group the rat is in (1 = orange, 2 = green, 3 = blue). Here it is clearly visible that the rats in group 1 are notably lighter than in the two other groups. 

Let us next make a weight profiles following the [MABS4IODS - chapter 8](https://mooc.helsinki.fi/pluginfile.php/192850/course/section/7335/MABS4IODS-Part6.pdf):
<br /><br />

```{r}

#Drawing a new plot using ggplot
ggplot(RATSL, aes(x = Time, y = Weight, linetype = ID, color = ID)) +
  geom_point(size = 0.5) + 
  geom_line(aes(group = ID)) +
  scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ Group, labeller = label_both) +
  scale_y_continuous(limits = c(min(RATSL$Weight), max(RATSL$Weight)))

```

<br />In the graph above we can first confirm that it really seems that the rats in the first group are lighter than in the other groups. In the second group most of the rats are weighting something between 400 g and 500 g except one rat which is approximately 100 g heavier. Overall the rats in third group are the heaviest, if the one exception from the group 2 is excluded.

In the **groups 2 and 3 all the rats are gaining weight**. The change in weight is nearly non-existent in the rats of group 1. In every group there is one rat which weight differs from the other rats > _an outlier_.
<br /><br />

### Summary measure analysis

<br />
The changes in the weight can be seen more clearly when the values have been **standardized**. In the following graph the weight of the rats have been standardized by subtracting the relevant occasion mean from the original observation and then dividing by the corresponding visit standard deviation. 
<br /><br />

```{r}

#Standardising the variable Weight
RATSL <- RATSL %>%
  group_by(Time) %>%
  mutate(stdweight = (Weight - mean(Weight))/sd(Weight) ) %>%
  ungroup()

#Plotting again with the standardized weight
ggplot(RATSL, aes(x = Time, y = stdweight, linetype = ID, color = ID)) +
  geom_point(size = 0.5) +
  geom_line() +
  scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ Group, labeller = label_both) +
  scale_y_continuous(name = "Standardized weight")

```

<br /> We can still move on from the standardized weight profiles. Individual profiles are usually not in the main focus and their interpretation is not that meaningful at this point. Instead we will produce a graph showing **average profiles for each group** at each time point. 

Thus, let us do that:
<br /><br />


```{r}

#Number of weighting times
n <- RATSL$Time %>% unique() %>% length()

#Summary data with mean and standard error of Weight by Group and Time
RATSS <- RATSL %>%
  group_by(Group, Time) %>%
  summarise( mean = mean(Weight), se = sd(Weight)/sqrt(n) ) %>%
  ungroup()

#Glimpse the data
glimpse(RATSS)

#Plotting the mean profiles
ggplot(RATSS, aes(x = Time, y = mean, linetype = Group, shape = Group)) +
  geom_line() +
  scale_linetype_manual(values = c(1,2,3)) +
  geom_point(size=3) +
  scale_shape_manual(values = c(1,2,3)) +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se, linetype="1"), width=0.3) +
  theme(legend.position = c(0.95,0.6)) +
  scale_y_continuous(name = "mean(Weight) +/- se(Weight)")

```

<br /> Above we have all three group averages in the same graph (see labels for the symbols on the right). 

There is no overlapping between the groups. Group 1 has notably lighter weight profile which is increasing in time just slightly. The weight profile of group 3 is little heavier than the one of group 2. Overall, the weight profiles of groups 2 and 3 are showing similar increase in time. **The error** is greatest in group 2 meaning that there is most variance and uncertainty in the results _(longest errorbars)_. The error in the group 1 seems to be smallest. 
<br /><br />

### Summary measure analysis - Boxplot
<br />

```{r}

#Creating a summary data by Group and ID
RATSSbp <- RATSL %>%
  group_by(Group, ID) %>%
  summarise(mean=mean(Weight)) %>%
  ungroup()

#Drawing a boxplot of the mean versus group
ggplot(RATSSbp, aes(x = Group, y = mean, col=Group)) +
  geom_boxplot() +
  stat_summary(fun = "mean", geom = "point", shape=23, size=4, fill = "white") +
  scale_y_continuous(name = "Mean weight")

```

<br /> Above we have a boxplot of mean summary measures for the three groups. The outlier in every group (discussed earlier) can now be seen as a dot (maximum or minimum observation of the group) below the boxes in the cases of _groups 1 and 3_ and above the box in the case of _group 2_ 

**The greatest outlier is in group 2** and it has quite large impact on its' error. Let us next **remove** the outlier by filtering out the weight exceeding 550 grams.
<br /><br />

```{r}

#Filtering mean weight values exceeding 550 g
RATSSbpf <- RATSSbp %>%
  filter(mean < 550)
glimpse(RATSSbpf)

#Drawing a boxplot of the mean versus group using the filtered data
ggplot(RATSSbpf, aes(x = Group, y = mean, col=Group)) +
  geom_boxplot() +
  stat_summary(fun = "mean", geom = "point", shape=23, size=4, fill = "white") +
  scale_y_continuous(name = "Mean weight")

```

<br /> Note how the boxplots of all three groups are now fairly similar together (size-wise). Removing the outlier makes it easier to see how the diet works for the majority.
<br /><br />

###  Independent samples t-test on the mean summary measure

<br /> Let us next perform a formal test for a difference, ergo apply **t-test** in order to assess differences between the three groups and to calculate a confidence interval for these differences. We shall use the averaged data without the outlier. 
<br /><br />

```{r}
library(rstatix)
library(ggpubr)

#Summary statistics of the data
RATSSbpf %>%
  group_by(Group) %>%
  get_summary_stats(mean, type = "mean_sd")

#Completing a pairwise t-tests
pwt <- RATSSbpf %>%
  pairwise_t_test(mean ~ Group, p.adjust.method = "bonferroni")

pwt

```

<br /> The outcome is a 3x9 table. The three groups are presented as rows and from the columns we can see the significance. There is a highly significant evidence for the groups to **differ** from each other.
<br /><br />


###  Analysis of covariance with baseline

<br /> Let us next fit a linear model with the mean as the response. From that we can compute the analysis of variance table for the fitted model using likelihood ratio test _ANOVA_. 
<br /><br />

```{r}

RATS <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/MABS/master/Examples/data/rats.txt", sep  ="\t", header = T)

#Adding the baseline
RATSL_B <- RATSSbp %>%
  mutate(baseline = RATS$WD1)

#Fitting the linear model with the mean as the response 
fit <- lm(mean ~ baseline + Group, data = RATSL_B)

#Computing the analysis of variance table for the fitted model with anova()
anova(fit)

```

<br /> The Analysis of Variance Table confirms that we have quite a **strong evidence of a treatment differences** between the groups, which confirms the outcome of the pairwise t-test. In fact, the differences could have been observed already from the first plots, but it is good to rely on reliable tests.
<br /><br />

## BPRS - dataset

<br /> Now we shall move on to the next dataset called 'BPRS'. In this dataset 40 males were randomly assigned to one of two treatment groups and each person was rated on the **Brief Psychiatric Rating Scale** _(BPRS)_ measured before treatment began (week 0) and then at weekly intervals for eight weeks. The BPRS assesses the level of 18 symptom constructs such as hostility, suspiciousness, hallucinations and grandiosity; each of these is rated from **1** _(not present)_ to **7** _(extremely severe)_. The scale is used to evaluate patients suspected of having schizophrenia.
<br /><br />

```{r}
BPRSL = read.table(file="BPRSL.txt", sep=",", header=TRUE)

#Factor treatment & subject of BPRSL
BPRSL$treatment <- factor(BPRSL$treatment)
BPRSL$subject <- factor(BPRSL$subject)

#Let us see the dimensions and structure of the dataset
dim(BPRSL)
str(BPRSL)
```

<br /> There are 360 observations of 5 variables. First there is the 'treatment' of the person, which indicates which one of the two treatments have been used on 'subject' (persons' "ID"-like number). The variable 'week' has been constructed out of 'weeks' and it tells which week is going on. The variable 'bprs' tells the brief psychiatric rating scale of the person.
<br /><br /> 

### Exploring BPRS by plotting

<br /> Let us plot the BPRS against the time (weeks).
<br /><br />

```{r}

#Plotting the BPRS data
ggplot(BPRSL, aes(x = week, y = bprs, col = subject)) +
  geom_point(size = 0.5) + 
  geom_line() + scale_x_continuous(name = "Week", breaks = seq(0, 8, 1)) + scale_y_continuous(name = "BPRS") + theme(legend.position = "top")

```

<br /> Putting all the observations in one figure is not very informative but fairly messy. Let us separate the subjects by the two treatments.
<br /><br />

```{r}

#Plotting the BPRS data once more
ggplot(BPRSL, aes(x = week, y = bprs, linetype = subject, col = subject)) +
  geom_point(size = 0.5) + 
  geom_line() +
  scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ treatment, labeller = label_both) +
  scale_y_continuous(limits = c(min(BPRSL$bprs), max(BPRSL$bprs)))

```

<br /> The graphs look a lot clearer now. We can already say that the **rate of BPRS is decreasing** for nearly all men during the eight weeks period. However, there is also some individual differences and variability. The overall variablity between the observations seems to decrease in time. There is not too much of rapid overlapping but the men whose BPRS is higher in the beginning seem to remain higher throughout the study. 

Let us next repeat the plot using **standardized** values of each observation averaged by the treatment.
<br /><br />

```{r}
#Number of subject
n <- BPRSL$subject %>% unique() %>% length()

#Summary data with mean and standard error of bprs by treatment and week 
BPRSS <- BPRSL %>%
  group_by(treatment, week) %>%
  summarise(mean = mean(bprs), se = sd(bprs)/sqrt(n)) %>%
  ungroup()

#Plotting the mean profiles of subjects
ggplot(BPRSS, aes(x = week, y = mean, linetype = treatment, shape = treatment)) +
  geom_line(aes(group = treatment)) +
  scale_linetype_manual(values = c(1,2)) +
  geom_point(size=4) +
  scale_shape_manual(values = c(1,2)) +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se, linetype="1"), width=0.2) +
  theme(legend.position = c(0.95,0.6)) +
  scale_y_continuous(name = "mean(BPRS) +/- se(BPRS)") 

```

<br /> Above we have both treatment group averages in the same graph (see labels for the symbols on the right).

There is now considerable **overlap** in the mean profiles. This may suggest that there is some differences between the two treatment groups with respect to the mean BPRS values. However, both treatment groups show decrease in the value of BPRS during the eight weeks period.
<br /><br />

### Linear mixed effects models for repeated measures data

<br /> Linear mixed effects model is suitable for responses that are assumed to be approximately
normally distributed after conditioning on the explanatory variables. This model formalize the idea that an individual’s pattern of responses is likely to depend on many characteristics of that individual _(including some that are unobserved)_. The unobserved variables are included in the model as **random variables** > _random effects_. The **essential feature** of the model is that the _(usually positive)_ correlation among the repeated measurements on the same individual arises from shared, unobserved variables. [MABS4IODS - chapter 9](https://mooc.helsinki.fi/pluginfile.php/192850/course/section/7335/MABS4IODS-Part6.pdf)

Let us next fit a linear regression model **ignoring the repeated measures** structure of the data. BPRS will act as response variable, and week + treatment as explanatory variables.
<br /><br />

```{r}
#Creating a regression model RATS_reg
BPRS_reg <- lm(bprs ~ week + treatment, data = BPRSL)

#Printing a summary of the model
summary(BPRS_reg)

```

<br /> In the results above 'intercept' and 'week' seem to be strongly significant. Instead, treatment 2 does not give any significant difference. However, the model cannot be trusted as we are aware that there is likely correlation between different measurements of the same subject. This correlation will be taken into account as _a random factor_.
<br /><br />

### Random intercept model.

<br />
Let us continue by using **a random intercept model**. The random component is assumed to be normally distributed and constant in time. Thereby, the linear fit for each subject is now allowed to differ in intercept from other subjects.
<br /><br />

```{r}

library(lme4)

#Creating a random intercept model
BPRS_ref <- lmer(bprs ~ week + treatment + (1 | subject), data = BPRSL, REML = FALSE)

#Printing the summary of the model
summary(BPRS_ref)

```

<br /> From the t-values above we can again confirm that there is greater evidence of a significant difference when it comes to 'intercept' and 'week', whereas treatment 2 remains insignificant. When we compare the standard error of 'week' we can observe, that it has decreased slightly from the previous model. This is due to **taking the time correlation into account**. In addition, the standard error of the 'treatment 2' has decreased. 
<br /><br />

### Random intercept and random slope model

<br />
We may still improve our model. The random intercept model does not always represent the observed pattern of variance and correlations between measurements in longitudinal data. Thereby, we will next fit a **random intercept and random slope model**. This allows the linear regression fits for each individual to differ in intercept but also in _slope_. This way it is possible to account for the individual differences in the BPRS profiles and also the effect of time. The two random effects are assumed to have a _bivariate normal distribution_.
<br /><br />

```{r}

#Creating a random intercept and random slope model
BPRS_ref1 <- lmer(bprs ~ week + treatment + (week | subject), data = BPRSL, REML = FALSE)

#Printing a summary of the model
summary(BPRS_ref1)

#Performing an ANOVA test on the two models
anova(BPRS_ref1, BPRS_ref)

```

<br /> From the table resulting from the ANOVA likelihood ratio test we can see that the p-value resulting from the chi-squared statistic is relatively small (0.02636 > 1% significance level). The low value implies that the new model (BPRS_ref1) **provides a better fit** against the comparison model (BPRS_ref).

Let us still improve the model with one more detail.
<br /><br />

### Random intercept and slope model allowing a treatment × week interaction

<br /> The final fit will be done with a **random intercept and slope model that allows for a group × week** _(time)_ interaction. 
<br /><br />

```{r}

#Creating a random intercept and random slope model with the interaction
BPRS_ref2 <- lmer(bprs ~ week * treatment + (week | subject), data = BPRSL, REML = FALSE)

#Printing a summary of the model
summary(BPRS_ref2)

#Performing an ANOVA test on the two models
anova(BPRS_ref2, BPRS_ref1)

```

<br />From the results of the likelihood ratio test of the interaction random intercept and slope model against the previous model we may observe that the **competence of the new model is actually worse**. The degrees of freedom have decreased from 2 to 1 and the chi-squared p-value is no longer inside the level of significance (from 0.02636 to 0.074, ergo significance worsens from 1% to 5%). Thus, the random intercept and random slope model _excluding_ the treatment x time interaction works better in our case.

Thus, let us continue with the previous model _(BPRS_ref1)_ in question.
<br /><br />

### Fitted BPRS profiles from the interaction model and observed BPRS profiles

<br /> Next, using the previous model _(the random intercept and random slope model)_ we will try to find and plot the fitted BPRS profiles, and then compare with the observed values for both of the treatment groups.
<br /><br />

```{r}

#Drawing the plot of BPRS with the observed bprs values
g_obs <- ggplot(BPRSL, aes(x = week, y = bprs, linetype = subject, col = subject)) +
  geom_point(size = 0.5) + 
  geom_line() +
  scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ treatment, labeller = label_both) +
  scale_y_continuous(limits = c(min(BPRSL$bprs), max(BPRSL$bprs)))

#Creating a vector of the fitted values
Fitted <- fitted(BPRS_ref2)

#Creating a new column fitted to BPRS
#BPRSL <- mutate(BPRSL, fitted = Fitted)

BPRSL <- BPRSL %>% mutate(Fitted)


#Drawing the plot of BPRSL with the Fitted values of bprs
g_fit <- ggplot(BPRSL, aes(x = week, y = Fitted, linetype = subject, col = subject)) +
  geom_point(size = 0.5) + 
  geom_line() +
  scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ treatment, labeller = label_both) +
  scale_y_continuous(limits = c(min(BPRSL$Fitted), max(BPRSL$Fitted)))

g_obs; g_fit

```

<br /> The model seems to **fit quite well** with the observed data. The linear model may not be the best option for this data but from the models tried above this one works the best. From here we can say, that both groups show clear decrease in the values of BPRS in time and that there seems **not to be notable differences** between the two groups. Both treatments worked equally well.
<br /><br />

***

<br /> 

> _Summary of my learning_

<br /> Phew, that was the last week! This demanded quite a lot work from me. It was odd to try to implement the methods to the opposite datasets. Some challenges were faced but I think I managed quite okay. I feel relieved that I got everything done and even though this week's tasks were quite heavy I actually enjoyed it (after I got the hang of it). 

<br /> _Thank you for the course_

<br />

Enjoy your holidays, you've earned it!

<br />

```{r}

ctree <- function(N=10){
  for (i in 1:N) cat(rep("",N-i+1),rep("*",i),"\n")
  cat(rep("",N),"*\n")
}

ctree()

```


***


Regards,
_Janina_




