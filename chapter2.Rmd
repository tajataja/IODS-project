# Chapter 2 - Regression and model validation

*Describe the work you have done this week and summarize your learning.*

***


> Hello there


Let us look closer to the data resulting from the Data Wrangling. 
<br /><br /> 

```{r}
date()


#Reading the data
learning2014 = read.table(file="learning2014.txt", sep=",", header=TRUE)

#Structure and dimensions of the data
str(learning2014)
dim(learning2014)
```


<br />The structure of the data looks good. There are 166 observations of  7 variable. The variables are _gender, age, attitude, deep (learning), stra(tegic approach), surf(ace approach)_ and _points_. The values of 'attitude', 'deep', 'stra' and 'surf' have been scaled by taking the mean. All the observations with points = 0 have been excluded.
<br /><br />

> Matrix plot

```{r}

library(GGally)
library(ggplot2)

# Creating a plot matrix with ggpairs(), colouring has been set by gender (M=blue, F=red)
p <- ggpairs(learning2014, mapping = aes(col=gender, alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))

# Drawing the plot
p


```
<br /> 
<br />So here we have a large plot matrix of all the 7 variables. Red colour indicates female answerers and blue men answerers. It is worth noticing that there are twice as many female answerers as men.

In the column most left we have the distribution of the observations presented in a horizontal way. The same distributions are presented also in a more visual way diagonally. The distributions of age and points are relatively similar between female and men answerers and the most differences can be found from distributions of attitude and surface learning. 

Below this diagonal, we have scatter plots between two variables and above the diagonal we have correlations between two variables in a numerical way. The higher the value of correlation is, the higher is the dependence between these two variables. In case the correlation is negative, the dependence is inverse: _(the value of one variable (y) decreases when the value of the other variable (x) increases)_.

The **highest** correlation is between points and attitude (0.437). The next highest correlation is **negative**, and it is between surface approach and deep learning (-0.324). The **weakest** correlation (-0.0101) is between deep learning and points.
<br /><br /> 

> Linear regression and correlation

```{r}
#y ~ x
#y - Target (dependent) variable: Exam points
#x - Three variables as explanatory variables: x1 -> attitude (highest correlation), x2 -> deep (lowest correlation), x3 -> surf (most  negative correlation)
```

<br /><br /> 
_EXAM POINTS ~ ATTITUDE_
<br /><br /> 
```{r}
#Creating a scatter plot y ~ x1 (positive correlation)
qplot(attitude, points, data = learning2014) + geom_smooth(method = "lm")
```

<br /> 
<br />So here we have a linear regression between attitude (explanatory variable) and exam points (dependent variable). The fitted regression line fits well and is directed upward, as there is a positive correlation between the variables. 

The significance can be observed from the summary of the lm-model. If we look at the p-value (Pr(>|t|)) we see that the value is really small (4.12e-09 << |0.05|). This indicates that there **is a statistical relationship** between attitude and exam points.
<br /><br /> 


```{r}
#Creating a regression model with the explanatory variables x1
my_model1 <- lm(points ~ attitude, data = learning2014)

#Printing out a summary of the model
print(summary(my_model1))

```

<br /><br /> 
_EXAM POINTS ~ DEEP LEARNING_
<br /><br /> 
```{r}

#Creating a scatter plot y ~ x2 (nearly non-existing correlation)
qplot(deep, points, data = learning2014) + geom_smooth(method = "lm")
```

<br /> 
<br />Here we have a linear regression between deep learning (explanatory variable) and exam points (dependent variable). The fitted regression line is nearly horizontal, which indicates that there is a weak correlation (if any). 

Now, if we look at the p-value (Pr(>|t|)) we see that the value is really fairly large (0.897 >> |0.05|). This indicates that there **is _no_ statistical relationship** between deep learning and exam points.
<br /><br /> 


```{r}
#Creating a regression model with the explanatory variables x2
my_model2 <- lm(points ~ deep, data = learning2014)

#Printing out a summary of the model
print(summary(my_model2))

```

<br /><br /> 
_EXAM POINTS ~ SURFACE APPROACH_
<br /><br /> 
```{r}

#Creating a scatter plot y ~ x3 (negative correlation)
qplot(surf, points, data = learning2014) + geom_smooth(method = "lm")
```

<br /> 
<br />Lastly we have a linear regression between surface approach (explanatory variable) and exam points (dependent variable). The fitted regression line is now negative. Thus, there is correlation between the variables, but it is inverse. 

Let us check again the p-value (Pr(>|t|)). We can see that the value is just outside the level of significance (0.0635 > |0.05|). This indicates that there **is _no_ statistical relationship** between surface approach and exam points. However, the p-value is so close to the level of significance that in some cases it could be taken into account _(this needs to be done with careful consideration)_.
<br /><br />

```{r}
#Creating a regression model with the explanatory variables x3
my_model3 <- lm(points ~ surf, data = learning2014)

#Printing out a summary of the model
print(summary(my_model3))


```

<br /> 
<br />To make the nexttask meaningful, I will be keeping two of the explanatory variables: _attitude_ and _surface approach_. The latter **does not have statistical significance** but I would not be able to do the multiple regression, if I would be dropping it off. 

Therefore, it stays.

As _deep learning_ has no significance what so ever, it will be excluded from the next phase.
<br /><br />


> Multiple regression

```{r}
#Creating a regression model with multiple explanatory variables
my_model4 <- lm(points ~ attitude + deep + surf, data = learning2014)

#Printing out a summary of the model
print(summary(my_model4))


```

<br /> 
<br /> _INTERPRETATION ????_
<br /><br />


> Diagnostic plots

_cheatsheet_

which | Graphics
----- | --------
**1**  | **Residuals vs Fitted values**
**2**  | **Normal QQ-plot**
3  | Standardized residuals vs Fitted values
4  | Cook's distances 
**5**  | **Residuals vs Leverage**
6  | Cook's distance vs Leverage


```{r}
#Creating a regression model with multiple explanatory variables
my_model5 <- lm(points ~ attitude + surf, data = learning2014)

#Drawing diagnostic plots using the plot() function. Plots 1, 2 and 5 chosen
par(mfrow = c(2,2))
plot(my_model5, which = c(1,2,5))


```

<br /> 
<br /> _INTERPRETATION_ 
<br /><br />


***

<br /> 
<br /> _Summary of my learning_ 
<br /><br />

***



Regards,
_Janina_
